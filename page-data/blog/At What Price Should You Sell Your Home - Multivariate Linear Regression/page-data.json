{"componentChunkName":"component---src-templates-blog-post-jsx","path":"/blog/At What Price Should You Sell Your Home - Multivariate Linear Regression/","result":{"data":{"site":{"siteMetadata":{"name":"Hemanth Kotagiri","title":"Hemanth Kotagiri | Full Stack Dev & Data Scientist","description":"Passionate Programmer 🧑‍💻 | Mathematics 🎲\n    | Philosophy 🦉| Physics ⚛ | AI 🤖 | Pythoneer 🐍 | Bibliophile 📚 |\n    Polymath 👁| Forever Learner   🧑🏻‍🎓| Excited Teacher 🧑‍🏫","about":"A freelancer delivering high-quality data-driven\n    solutions for challenging problems such as image classification, regression\n    analysis in the field of Machine Learning and Deep Learning. I love working on\n    web-development and mobile-application development projects as well. In my\n    fair time, I tend to ponder upon the universe, existence, consciousness,\n    science, psychology, philosophy, physics, mathematics, and learn literally\n    anything that crosses my mind.  Yes, an aspiring polymath.","author":null,"github":"https://github.com/hemanth-kotagiri","linkedin":"https://www.linkedin.com/in/hemanth-kotagiri/"}},"markdownRemark":{"id":"fadb3632-bd26-57a8-bddc-5fe840104e52","excerpt":"Prerequisites \nIf you haven’t read my previous article, in which I walk you through step by step behind the scenes of the Linear Regression Algorithm, where we…","html":"<h2>Prerequisites</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a2a23e2acb4ffe34d4d9bb27601f971b/d2c28/20210628231947.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.89189189189189%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAADX0lEQVQ4yx2T+09aBxTH77KlqbFDEB+IApf3S0BAuFB5XkCeIgKKigo+iSLWGFe7VLu1M1uTptn2y5asW5b9smR/5Wd3+/mcfJ/nCFLES209SiqboFQoEIolWUoWiKVzrDZb3NxdstssslHMcrm/icPloSRnePfwmp9+/sgvv/3Od4/v6TYb9No1BDkn47Fbifpc9LsttjodLs5PkStVzl6+4Oj8iFQ8xM5GlXoxRy61QizoZvV58P/du7tXfLi/4Oa0xctRD2G3USAhebgaFlkOuWltbvL28ZFyo4lcLlOulRFdQQrVOuFImFeXHT48nNBrVVgy64m6jDTyEvFlK73+NkJFYZUCVlr1IEabl2xumXpjhWQqzcXomovhkHS2yIvzE6LBCLbZGXYqBa6P++z1T9gbjLi7f0O+lMfksCM43AakkI9MOs1yLEE87SasKF70eNhq1zkc9Qln8zSrWTKFPMf9DvdfDZhXT9C/vuDr798wHA1ZPzhDii4hlBseIisBDKIJm9VCPiUxOztPQsnqanjAvMWGOyFz/PpbWr0e5fUVKqsZwmE/Vu8CkaiT8t4ZmU6bZMmB4A5r8Ut6xPkx5JSL47MCxaLE1dUpP7x/h8XqIBxcYufwiEjcT9C/gE4vEggogGYbDpvixiui0T1hYk6FMLMwjjswS0I2IFqmyNUsyHkXibjE24db3P4ARqOZwX6L9bUUYZ+JfiNJp5YllUxhFy0KiQu9YQadUcnQK6kxu1Rs7FpwBaZZq1oprYbQaHS012S621VEp52VTIKt7ia6eSOS30r5uUMhEnFaTGhVX+K1mfAtehD2Rwb0JjVOv4Z6V1TUOZEzMdrtIp3uNnuHfUW1l9tv2oTicRZtogK2qBTpJZaR+fXTj/QP2vh9boa3VwgbB0YKDRdPxsdRz4yh1k0wOadhMGhzrjRsczjR6WZolCJkJAdm8zQBv8jBZplELk9tt8v1zYC///mLT3/+gWAPKLlVvUxM6XiqGlPsTVDfsZNIL2Ox2Ujnc+jm/ivCRDwZUo5cSyRtot2ME3CKqDSTqCe1rNdLZNNxBKt7gVx9EbPdiEb9jHHNU4wWLXqzgc8+H0NK+akqv2xyOPji2ZgyH2POpGJwGSNf8aGansXg82HS6ZnTTvEvpz3COrjJBoQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628231947\"\n        title=\"20210628231947\"\n        src=\"/static/a2a23e2acb4ffe34d4d9bb27601f971b/fcda8/20210628231947.png\"\n        srcset=\"/static/a2a23e2acb4ffe34d4d9bb27601f971b/12f09/20210628231947.png 148w,\n/static/a2a23e2acb4ffe34d4d9bb27601f971b/e4a3f/20210628231947.png 295w,\n/static/a2a23e2acb4ffe34d4d9bb27601f971b/fcda8/20210628231947.png 590w,\n/static/a2a23e2acb4ffe34d4d9bb27601f971b/efc66/20210628231947.png 885w,\n/static/a2a23e2acb4ffe34d4d9bb27601f971b/c83ae/20210628231947.png 1180w,\n/static/a2a23e2acb4ffe34d4d9bb27601f971b/d2c28/20210628231947.png 4000w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\nIf you haven’t read my previous article, in which I walk you through step by step behind the scenes of the Linear Regression Algorithm, where we intuitively build up the entire essence of Math that goes behind it incrementally, you really need to check that out - <strong>Introduction to Machine Learning</strong>. This article is an extension of the previous one and I assume you have had a go at it. In this article, let us deal with the problem where we have multiple features/variables that influence our algorithm. Again, these are the personal notes that I have taken while going through Andrew Ng’s Machine Learning course over at Coursera. Feel free to pick that up if you are interested and the best part is that the course is free.</p>\n<h2>BACKGROUND</h2>\n<p>Notice the picture that I have specifically chosen for this article — Houses. It’s apparent that each home has the potential to be sold at a valuable price. In the previous article, we have dealt with a single feature — That is, we literally built our entire algorithm based on a single factor that might influence the price of a home. Let me make things even more clear of what we are referring to when we say “Features”.</p>\n<p>Say you are a resident in one of these homes. And you are planning to sell your home. How would you go about effectively choosing a threshold to which you are supposed to sell? That’s where this algorithm can help. The features, in this problem, are the characteristics of your home. Such as :</p>\n<ul>\n<li>How old is the home?</li>\n<li>The total number of bedrooms that your home has.</li>\n<li>The number of bathrooms that your home has.</li>\n<li>The total area that your home occupies.</li>\n<li>Does it have a parking lot? If so, how many cars can I park?</li>\n<li>Does it have a backyard where I can sit back in the monsoon and read a romantic novel?</li>\n<li>Does it have a swimming pool where I can enjoy the summer?</li>\n</ul>\n<p>You can already see, the feature space is blowing up and you can also combine a couple of features and create a new one as well. This is where the feature space comes into the picture and it’s really essential to keep track of what features influence the most, and which do have a greater impact on the performance of the algorithm in the long run.</p>\n<p>So, the idea is, to predict the price that your house can be sold based on the “learned examples” which had the same multiple features. If you have understood the above, congratulations! You can now start working on the most famous Kaggle Competition that all beginner Machine Learning aspirants take up — The Housing Prices Competition — This was my first competition as well. Enough of talking, let’s get to Math.</p>\n<h2>HYPOTHESIS FUNCTION — REDEFINED</h2>\n<p>Here, based on the hypothesis we defined in the previous article, if we have a function with multiple variables/features such as x1, x2, x3… xn; then, we would represent the hypothesis function like :\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 441px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ce257271f8d41797d8e17761eb242932/efc6e/20210628232207.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 12.837837837837837%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsSAAALEgHS3X78AAAAhUlEQVQI112O3QrDIAxGff8nXFlnWS+UqbUrXhj8+2YcHaOBYM4xCRFEhJwzWmsgikgp/bLWilJKz4wY42D27DiYef7bUwaLabpBK4UQAqR8wJgXtNbDbZvrbOC9xzzf4ZzFuj5xHO+xiA9YFglrTZ9R2HcPwZfxZn6vcfVcn8x///7s/QA1fujRvxvMYwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628232207\"\n        title=\"20210628232207\"\n        src=\"/static/ce257271f8d41797d8e17761eb242932/efc6e/20210628232207.png\"\n        srcset=\"/static/ce257271f8d41797d8e17761eb242932/12f09/20210628232207.png 148w,\n/static/ce257271f8d41797d8e17761eb242932/e4a3f/20210628232207.png 295w,\n/static/ce257271f8d41797d8e17761eb242932/efc6e/20210628232207.png 441w\"\n        sizes=\"(max-width: 441px) 100vw, 441px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>We can choose to represent our hypothesis function in the form of vectors, in many programming languages such as MATLAB and Python, using a vectorial representation increases the computational speed due to the way in which they are designed.</p>\n<h2>GRADIENT DESCENT FOR MULTIPLE VARIABLES</h2>\n<p>Let n denote the number of features given in the data set. Previously, we had n = 1, which is just one feature, but in this, we may have more than 2 features. So the Gradient Descent algorithm has a minor change such as this:\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 522px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c3f3fcb69aa477599049a882212464a3/29492/20210628232255.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 18.243243243243242%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsSAAALEgHS3X78AAAAlElEQVQY06WQ2wrEIAxE/f+/7LaW6ha8X0rR2Rja0vcdCJoJOYMK7x3ksmDbFNZ1hVIaUkporRFTQmsNt3rvXFfD9zF/lwghwBgDay0yAaZpwmeesX939mqt8N4jBI/zPHFQH2NEKYVnT8AlkVPmQcmZgTPBlFIcUshP5A3ACB7LlUDOOWQ6h3+/4AaLWg/8q/dX/AD4hTflfwSJtgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628232255\"\n        title=\"20210628232255\"\n        src=\"/static/c3f3fcb69aa477599049a882212464a3/29492/20210628232255.png\"\n        srcset=\"/static/c3f3fcb69aa477599049a882212464a3/12f09/20210628232255.png 148w,\n/static/c3f3fcb69aa477599049a882212464a3/e4a3f/20210628232255.png 295w,\n/static/c3f3fcb69aa477599049a882212464a3/29492/20210628232255.png 522w\"\n        sizes=\"(max-width: 522px) 100vw, 522px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>The following image compares gradient descent with one variable to gradient descent with multiple variables:\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 581px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/0319ad7dca1784b8c371241b82d599b3/92d15/20210628232303.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 54.72972972972974%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAACHElEQVQoz21TyZabMBD0//9PDvmAHJJM4ok3bBYbr4ANAoZFQiyVkvw8l4T3CiGp1V1VamZCCGwdB77vIwgCeJ6HJLnDPNM4YjIfdYXU3WHnuozd4nK94SPPUd+u6MsCkBLTR4lpGjGrqwqu68HfH+DzgEdUUj0TvlDkyLIMYRhCiBwwhbiGcwg4K2D+E/C2ANdmehiwWy7hbzbIfRfK3VpGyMUTDBq1huZ317aoWwlVFHgw7h7HaESGjiorMm6PB8ympka6WUGczxiqD0hWui4XaC5nyCvBoM7boTOyyGycJvRMfHmfI6Y9xemInCgpX0Y3zCBbS18RHX2oeDjaOih3DtpDwMQn6MCD6joMk3UUPVXFUYwkfkLc76icNTqynL1MR55ZeaDJ/eoP4q9fUPz6YYsZyTXHmod1+sDI2EGkjH3GD0b2co6Re5Zhb5hw0tU1GvpzOexRxREU1wzzitLb+RuawEdFFS3jzscj0iRhoQJNWaL4/Qa9XjJhr1lRWKN7BhdRhGgfoExTNKsFKqKhZM0L6zn29HQkEvrekIgO9+izFPIUYuDcSh7pCZQE2sYmfNDklJd091zk14tlYfxUTKTIvHvccVu8I99uIPc+JPdqzof1gglptFLqs+f+95h1uZhDslVak5CMTps1xDG0c0mYW9bfv1Hy6xAT/wO2CV/gG+p8tLI1xz6JbDeYvhvosflbNC9motd/AVO5PsZkTv7IAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628232303\"\n        title=\"20210628232303\"\n        src=\"/static/0319ad7dca1784b8c371241b82d599b3/92d15/20210628232303.png\"\n        srcset=\"/static/0319ad7dca1784b8c371241b82d599b3/12f09/20210628232303.png 148w,\n/static/0319ad7dca1784b8c371241b82d599b3/e4a3f/20210628232303.png 295w,\n/static/0319ad7dca1784b8c371241b82d599b3/92d15/20210628232303.png 581w\"\n        sizes=\"(max-width: 581px) 100vw, 581px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>Note the variable “j” that goes from 0 to n — that is, it iterates all the way from the bias term till the last feature(n). We compute the partial derivatives with respect to each of the features and we update the theta values — the parameter vector. There’s an issue here, do you notice it? We are iterating the entire feature space, computing the gradients, updating the theta values in a single step. And, we inevitably repeat this until the change between these theta values is significantly low. Now, that makes Gradient Descent perform slow. Like, really so slow that you get the urge to just stop learning Machine Learning and go back to your comfort zone. Just kidding, let’s look at a few ways we can improve the overall speed of the optimization algorithm.</p>\n<h2>HOW CAN YOU SPEED UP GRADIENT DESCENT?</h2>\n<ul>\n<li>We can speed up gradient descent by having each of our input values in roughly the same range so that the contour plots which are highly skewed can be brought down to circles so that the gradient descent would find it easier to get to the global/local minimum.</li>\n<li>We can ideally choose the range of our input values to be between -1 and 1. We can modify the input feature(s) values so that all of them roughly fall in the same range throughout the entire dataset.</li>\n<li>For this, we can use one of the two techniques among a gazillion others. Feature Scaling or Mean Normalization. We can go about writing a whole book upon just this single step, no wonder, there are many already out there. Anyhow, let’s look at what do these two techniques do.</li>\n<li><strong>Feature Scaling:</strong> In this method, we divide the input values by the range (the maximum value minus the minimum value) of the dataset resulting in a new range of just 1.</li>\n<li><strong>Mean Normalization:</strong> In this method, we can subtract the average value of the input feature with the input variable and divide it by the standard deviation or by the range of the input feature. Note that dividing it with the standard deviation with yields different results compared to diving it with the range. For example, if xi represents housing prices with a range of 100 to 2000 and a mean value of 1000, then xi = (xi— 1000)/1900.</li>\n</ul>\n<p>Now that you understand a couple of steps you that can take to speed up Gradient Descent, how would you go about ascertaining the convergence of the optimization? Did it really learn the parameters, did it reach the global minimum yet? Or is it taking tiny, baby steps still towards the global/local minimum? Or did it just shoot off the contour plot to infinity? Let’s answers these questions</p>\n<h2>DEBUGGING THE GRADIENT DESCENT</h2>\n<h4>The visual way — Plot the graph</h4>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 500px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8d5a41143f943d54509b872dbf12aac2/0b533/20210628232404.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75.67567567567568%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAABVUlEQVQ4y62T2W7CMBRE+f9/4wEhkRcWUUAkwTT76tjDXAcRKKVNKyw5suXo3Bnf8QQcXdfhHcNai4ksgiBAHEdo2xZ1Xf9pNk2DqqpgjHFQByzLArtdQqXGHYyd4kxUCVhrPQBPpxCeF/3bqji7AaVCksRYLlP0qq2rOnbKENsPCvM8oeWU4HvguCY8AbXucD4H2G4/2ZgBOLarT8DD4UCFKZQqCH6DwrIs4fs+oijC8fgGhfLJspwqM4QhGAF7heJXlS+BZ3rNshhkMkI9UDreQ4cr+G79osu5syxjv7c3lffza1yMGYB13bgs3oCSdIH2IQXYJ8iWhSGFe7X2B+vd49MTuUHgO+ltW/MuFZVWBGsGPua65bnBep0wXiX3GouFwmqVubXnhYxcMgCFXhTF9X2aq33r7nU+nxHyQaDCdDrDZrNzqsMwQppWzpFSMf/NHPACMPeZoUQKYb0AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628232404\"\n        title=\"20210628232404\"\n        src=\"/static/8d5a41143f943d54509b872dbf12aac2/0b533/20210628232404.png\"\n        srcset=\"/static/8d5a41143f943d54509b872dbf12aac2/12f09/20210628232404.png 148w,\n/static/8d5a41143f943d54509b872dbf12aac2/e4a3f/20210628232404.png 295w,\n/static/8d5a41143f943d54509b872dbf12aac2/0b533/20210628232404.png 500w\"\n        sizes=\"(max-width: 500px) 100vw, 500px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\nOne effective way to visually check if gradient descent is working properly is by plotting. To debug the gradient descent, make a plot with the number of iterations on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α.</p>\n<h5>Automatic Convergence Test</h5>\n<p>Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as 10^−3. However, in practice, it’s difficult to choose this threshold value.\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 562px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/99874912658b2508ba20a37ddf111a07/6e88f/20210628232432.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 52.70270270270271%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAABzUlEQVQoz3VSi3KiQBDk/z8tUnWCEhGCJBEEBVEiuAvC9vUuF88kla0ahuLR09Pd1syewV+tsNlEsGczOPM5FgsXrvMHURTC959h2zZW/Ga5XMJxFrjdbtBHKWXq8VhheCTYHq+vJ5xONaQUyLIrdruWJZEkAte2weVygRACTdNgGIYvgI+gVhTVyPOKgPX9YRiCbICiAIF6/Ha+szOAdd2jJpaU+meFcVRkrBDHimwUh3XYbksjg147WAdc28E2SX+srbvVNFd8fAzoe4WnpxHns0LXKVTVVEnSoT7XiF5e4HnPeH9/M6BSyAem5mo6GQoyGw27w0EZQP1SszseFQ2QxoSu64x2+n4Ybqb3fceSdzDdraIQZkqegyBT19W2oEn6WY9NtIfLlYMgZK0xZxIcpsBdxLzPKdf4f+Usazn9ahwsS0lnBfUTjIygOYIf92RKDV3XaLf0PKx9H8UhQ8CEeJ7kcPWPJQFbE4kGaZqQ0YFrZyjKHbUrCZyS4cWsuyftgranaYqzps4zklhVfdXR0mZoh+O4ZMB3jMsbw5xQ/JFsQTD5a2R0IqTUzMa70zRFcV0tPpjFaaImkOeKjKYfPtf5XhPwYEL/Gfa/c3JICnvJyh4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628232432\"\n        title=\"20210628232432\"\n        src=\"/static/99874912658b2508ba20a37ddf111a07/6e88f/20210628232432.png\"\n        srcset=\"/static/99874912658b2508ba20a37ddf111a07/12f09/20210628232432.png 148w,\n/static/99874912658b2508ba20a37ddf111a07/e4a3f/20210628232432.png 295w,\n/static/99874912658b2508ba20a37ddf111a07/6e88f/20210628232432.png 562w\"\n        sizes=\"(max-width: 562px) 100vw, 562px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>It has been proven that if the learning rate α is sufficiently small, then J(θ) will decrease on every iteration. That doesn’t mean to choose an extremely low learning rate which will potentially slow down gradient descent even more. if α is too small: slow convergence. If α is too large: it may not decrease on every iteration and thus may not converge. So, choose the learning rate alpha wisely depending on the data-set you are dealing with. Tinker around and plot the cost, you will clearly see which alpha value is ideal for your dataset.</p>\n<h2>POLYNOMIAL REGRESSION</h2>\n<ul>\n<li>Our hypothesis function need not be always linear to fit the data reasonably. In fact, many datasets out there follow a polynomial trend over a linear trend. We can choose to include polynomial terms as well in the function to hopefully get a good hypothesis.</li>\n<li>We can improve our hypothesis function by combining multiple features into one. For example, we can combine x1 and x2 features as x1<em>x2 and make a new feature x3 = x1</em>x2.</li>\n<li>Remember, when you are including polynomial terms — feature scaling becomes very important. Because some of the features may shoot to a very large value making the curve quite unstable. Say including the feature x3 = x1*(x2³). In this case, if the value of x2 is 100, and x1 is 10, then the value of x3 will be 100,000,000 which is very huge. In this case, feature scaling comes handy to drop that value to a desired range so that the curve doesn’t act unstable.</li>\n</ul>\n<hr>\n<h2>A SNEAKY WAY TO COMPUTE THE PARAMETERS — AVOIDING THE GRADIENT DESCENT</h2>\n<ul>\n<li>We can also compute the parameters vector theta using analytical techniques such as using this: theta = inv(X^T _ X) _ (X^T * y). This is known as the Normal Equation for linear regression.</li>\n<li>There is no need to do feature scaling while finding the parameters analytically. Below shows an example for 4 training examples.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/1b67408b5a92f4ac5a6cf65a3b07ac46/1ddef/20210628232606.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 54.72972972972974%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAAB+ElEQVQoz5WTW3ObQAyF/f//VNu3PnSSTJO2SYxtCBeDWS4Li7nvVwFxpp0+dRkhDas9e6Qjdl3bEoYhb76PLjVdXWPHkf9d1trV74ZhQKVqNVNVqNMBFQZUAlzqkqZpVl/VFbV801qv/ipEyqIkjhNaiW+gu9sNc5XDNJNmObkk+p7Pl0+f8VwPZ7/n6fEHla54uLvHeXUwxpCkCfE5puv6vwHLoeI+eyJuU46Zy6/zK4E5S+zh6QC/jjhIHHcp+/SIVwbotlr3DspjNBqkTUvRu+XdM3BpFEusEjn0a888TFzNlbZpmfoRow3zOFOoXIBCmqklSiNewxO9kbNttwH2VnpIyckEZOJflMPX4x3ZXBKZhEuXEY4X2fcpbMVRGHlVIBRGHoqffD8/M9fSru4dsLEtudVUNChbogTIu4bssxO+jlBjwXPh4NY+IsVqgx05xdJb7a7VLUB2a+LWw8725AIZj4vSBlPURH7EJUoYrz2RF1KqYlNvsqsgBxEmj7P3mdnMyrO7zc/yYWRiGLp1fFJRMCli9GQ4S5wX+Zo2CjvTNbihR5hFtMJxqXIhtWDs/hzKWaU0by7l8UgWODinbyT+I3PbLNV85C1MzpPiZXIJ7eXd0m2wl6QPQBneIc/oMsVYFEyiqM2krGm6/Q4fuf+sebvwN+iqTDfBUaElAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628232606\"\n        title=\"20210628232606\"\n        src=\"/static/1b67408b5a92f4ac5a6cf65a3b07ac46/fcda8/20210628232606.png\"\n        srcset=\"/static/1b67408b5a92f4ac5a6cf65a3b07ac46/12f09/20210628232606.png 148w,\n/static/1b67408b5a92f4ac5a6cf65a3b07ac46/e4a3f/20210628232606.png 295w,\n/static/1b67408b5a92f4ac5a6cf65a3b07ac46/fcda8/20210628232606.png 590w,\n/static/1b67408b5a92f4ac5a6cf65a3b07ac46/1ddef/20210628232606.png 635w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 551px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/72cab5af573862888d12bc56e773e6df/db783/20210628232618.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 33.108108108108105%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsSAAALEgHS3X78AAAA9klEQVQoz11RibKEIAzz/39zd563CIqAB9kGF8a3nemgNE3TUDnnsG0beHrvcZ4nYoy4rislv3mX685tgrcIwYORcTmrcRzAnKYJfd+nxgyM8SY8jgNKTYIboRKuE/yIdV3L0CyiGoYvoYCbpoa164MwFoVKKYyC5Uky9rVtUzYqhE1dJ2Usvl8vzPP8s/IlCvdCwJODQwjI8Y+wa1t0XZd+npFBt8JDBiloPcMYk0jzZrddVNyLBQsqTmXSQ2sttCi04k2emD3kFmxkEl/Xf1iXJT2o9y4p3vcdFT3RWifgIgD6RBVPH+kTMazzIYzRyUve/270AWYJHjZ0XrcZAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628232618\"\n        title=\"20210628232618\"\n        src=\"/static/72cab5af573862888d12bc56e773e6df/db783/20210628232618.png\"\n        srcset=\"/static/72cab5af573862888d12bc56e773e6df/12f09/20210628232618.png 148w,\n/static/72cab5af573862888d12bc56e773e6df/e4a3f/20210628232618.png 295w,\n/static/72cab5af573862888d12bc56e773e6df/db783/20210628232618.png 551w\"\n        sizes=\"(max-width: 551px) 100vw, 551px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2>Putting it all together</h2>\n<p>If you have made it this far and understood most of it, congrats and give yourself a pat on your back — because you deserve it! You have learned everything that you need to practically apply these concepts to a real-world dataset and generate a predictive model. You have learned about the Cost function, the optimization algorithm — Gradient Descent, computing the parameters analytically, choosing the learning rate effectively, and also debug your algorithm’s performance. Stick them all together in your favorite programming language — You have linear regression.</p>\n<p>It was a great journey so far, and you have just breezed through the tip of the iceberg. There’s a lot more you can learn, and believe me, there are many researchers out there who spend their entire lifetime squeezing the point one percent performance out of each of these algorithms, trying different ways to implement hoping for an improvement over the previous version. So, don’t worry, you can get through it with a basic understanding of these concepts.</p>\n<p><em>So, never, ever stop learning — no matter what!</em></p>\n<blockquote>\n<p>For Precious, with Patience.</p>\n</blockquote>","frontmatter":{"title":"At What Price Should You Sell Your Home - Multivariate Linear Regression","date":"May 15, 2021","description":"Understanding Multivariate Regression"}}},"pageContext":{"slug":"/blog/At What Price Should You Sell Your Home - Multivariate Linear Regression/","previous":{"fields":{"slug":"/blog/Introduction to Machine Learning/"},"frontmatter":{"title":"Introduction to Machine Learning"}},"next":{"fields":{"slug":"/blog/Gravity - Sounds simple and familiar/"},"frontmatter":{"title":"Gravity - Sounds Simple and Familiar?"}}}},"staticQueryHashes":["63159454"]}
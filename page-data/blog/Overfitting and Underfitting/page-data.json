{"componentChunkName":"component---src-templates-blog-post-jsx","path":"/blog/Overfitting and Underfitting/","result":{"data":{"site":{"siteMetadata":{"name":"Hemanth Kotagiri","title":"Hemanth Kotagiri | Full Stack Dev & Data Scientist","description":"Passionate Programmer üßë‚Äçüíª | Mathematics üé≤\n    | Philosophy ü¶â| Physics ‚öõ | AI ü§ñ | Pythoneer üêç | Bibliophile üìö |\n    Polymath üëÅ| Forever Learner   üßëüèª‚Äçüéì| Excited Teacher üßë‚Äçüè´","about":"A freelancer delivering high-quality data-driven\n    solutions for challenging problems such as image classification, regression\n    analysis in the field of Machine Learning and Deep Learning. I love working on\n    web-development and mobile-application development projects as well. In my\n    fair time, I tend to ponder upon the universe, existence, consciousness,\n    science, psychology, philosophy, physics, mathematics, and learn literally\n    anything that crosses my mind.  Yes, an aspiring polymath.","author":null,"github":"https://github.com/hemanth-kotagiri","linkedin":"https://www.linkedin.com/in/hemanth-kotagiri/"}},"markdownRemark":{"id":"a1bae32c-fcf0-5f38-b117-02b8f185783b","excerpt":"In this single article, let‚Äôs address the problem that plagues all of Machine Learning. I assume you have read my previous article(s) wherein I introduce you to‚Ä¶","html":"<p>In this single article, <strong>let‚Äôs address the problem that plagues all of Machine Learning.</strong> I assume you have read my previous article(s) wherein I introduce you to Machine Learning and walk you through a few classic algorithms. The prerequisites for this article are that you should be familiar with at least a single algorithm, be it Introduction to Machine Learning, At what price should you sell your home - Multivariate Linear Regression, Lets Learn the Fundaments of Classifying Cancer - Logistic Regression, or others. I have linked the articles and you can click and read through them! ‚Äî Anyone from the above would be sufficient to understand the concepts laid out in this article. Let‚Äôs get started!</p>\n<h2>Introduction</h2>\n<p>Alright, so you have gathered all the data that you need and preprocessed it, you took care of all the features that you would like to include and remove those that you don‚Äôt. Before we go directly into the problem, let‚Äôs learn about some technical jargon regarding data so that we are on the same page.</p>\n<p><strong>Training Data/Set</strong> is the collection of data points and it‚Äôs the starting point that you train your algorithm upon. Sounds simple? Yes, you use this data to learn and recognize a pattern from the data and hopefully form a good hypothesis.</p>\n<p><strong>Validation Data/Set</strong> is the collection of data points that the algorithm you‚Äôve trained has never seen before. You decide the performance of your algorithm based on the predictions that you gather from feeding this data to your model. Solely based on the performance of your model upon this set, would you be able to decide if it‚Äôs working as expected or not.</p>\n<p><strong>Testing Data/Set</strong> is like the last resort. Of course, your model doesn‚Äôt know these data points, but this is the final evaluation step. You tweak your model based on the insights you‚Äôve gained from the previous set‚Äôs performance and test the algorithm upon this set.</p>\n<p>Why am I talking about all these? You see, when you train an algorithm ‚Äî be it a regression problem statement or a classification, the data is all that determines how good your model can work in the long run. As the quote goes:</p>\n<p><em>In the end, it is not the one who has 99% accurate model that wins, but the one who has a lot of data.</em></p>\n<h2>Overfitting &#x26; Underfitting</h2>\n<p>Now that we have a brief idea of the technical terms such as Training, Validation, and Testing sets ‚Äî for the sake of simplicity, let‚Äôs focus the performance of the algorithm on the first step, which is the training set. Assume that you have trained a model three times. After each time you trained, you have checked the performance of your algorithm by plotting the best-fit curves against a few data points. And these are the graphs you‚Äôve obtained:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 540px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/751bea99657a3f2e90eac0eb4db019a4/07484/20210628234546.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 27.7027027027027%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsSAAALEgHS3X78AAAA6UlEQVQY02WR0W6DMAxF8/8f2IdKqExa+1BCSHASAgRubXdMo7Nk2bnHDnYw1lrEmEBEHAnORTweAdb2nDvWourjSOh74YS2/ebof5n0eh/RdQNMzhMOSwmY+FjrhmEYMM+z6hKElQIsC3C5fHHdqkzqhUmNcytMSlGLQthY3LVoXRedsJSsDUQb9l3Ym1+vN564KJum/YdBNWNt1i//tVorT+j4onfTp7W3ltdb/vXJ1Eb2rlVgUZc1E+/wfHa434OeD3bwpml4vZHzcx/RCBO85wcN7P7kMWae0vNTnPUQgv4E0SX/7HsB1GvQG2GKaPwAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628234546\"\n        title=\"20210628234546\"\n        src=\"/static/751bea99657a3f2e90eac0eb4db019a4/07484/20210628234546.png\"\n        srcset=\"/static/751bea99657a3f2e90eac0eb4db019a4/12f09/20210628234546.png 148w,\n/static/751bea99657a3f2e90eac0eb4db019a4/e4a3f/20210628234546.png 295w,\n/static/751bea99657a3f2e90eac0eb4db019a4/07484/20210628234546.png 540w\"\n        sizes=\"(max-width: 540px) 100vw, 540px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>From the above picture, you can draw a few key insights. They are:</p>\n<ul>\n<li>First Plot: We are clearly underfitting the data. In this case, we are using a linear function that doesn‚Äôt give the optimal prediction for new data points. Notice that whatever you do, the curve is linear when you do not include the polynomial terms and the cost shall remain the same with the best fit line. This can be resolved by including a few polynomial terms making it a non-linear function and hopefully get a better hypothesis. Naively, it might seem that the more features we add, the better the fit to the data. However, this is not the case and you‚Äôll see why.</li>\n<li>Third Plot: If we add too many features, we eventually end up overfitting the training data which is not optimal because if we do so, for the testing data or even the validation data, the output wouldn‚Äôt be as close to the true value.</li>\n<li>Second Plot: This is the ideal curve and it is the right hypothesis for the training data provided which doesn‚Äôt overfit or underfit it.</li>\n</ul>\n<p><strong>Underfitting</strong>, or high bias, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features.\n<strong>Overfitting</strong>, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.</p>\n<h2>But, How do we overcome these two issues?</h2>\n<p>Simply <strong>when you are faced with underfitting</strong> ‚Äî You can resort to adding more features and also include a few polynomial terms or even combining existing two features and making up a new one. You are free to do that, but it does come at the cost of Overfitting.</p>\n<p>In the other case, <strong>when you see your algorithm is overfitting</strong>, there are two of the most common ways you can resolve it. The first one is to simply reduce the number of features that you include for training and manually select the features carefully. The other one is Regularization. The main aim of Regularization is to keep all the features, but reduce the magnitude of parameters Œ∏j. And, it works well when we have a lot of slightly useful features. Let‚Äôs talk about this in the contest of both Linear and Logistic Regression.</p>\n<h2>Regularized Linear Regression</h2>\n<h4>Cost Function</h4>\n<p>When we choose to regularize our algorithm, there are a few changes that we need to make. If we detect overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost. Say we wanted to make the following function more quadratic:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 249px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8699a35edcac035f423b5468a882988a/6a5fb/20210628234716.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 12.837837837837837%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsSAAALEgHS3X78AAAAo0lEQVQI11XOXQ+CIBiGYf//b7OTdDqnkAc5l6aggvHZE1KtxcYB19j73gk+RykF59z3CWstTLjee3DO32b0z5Y1mtY6/HV4egO5SyQjYxBiRV2VyMoKfd9DbhuuXYdTmoI2BLfhDikkWlLjnOdoaItxGCClAKEURVbgQhpM84REBLShzGoFtq54HKWh4jAelm1CxBJjTKxe+GHyz9g0h+F7tBdoaeXplro4+AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628234716\"\n        title=\"20210628234716\"\n        src=\"/static/8699a35edcac035f423b5468a882988a/6a5fb/20210628234716.png\"\n        srcset=\"/static/8699a35edcac035f423b5468a882988a/12f09/20210628234716.png 148w,\n/static/8699a35edcac035f423b5468a882988a/6a5fb/20210628234716.png 249w\"\n        sizes=\"(max-width: 249px) 100vw, 249px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>If we want to eliminate the influence of the last two terms without actually removing them, we can modify the cost function as follows:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 454px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/aae908c841ad642cd38d97482c540e07/b3c1d/20210628234740.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 13.513513513513512%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsSAAALEgHS3X78AAAAf0lEQVQI13WOQRLDIAhFvf8R27o36EITFlHRX3CaNpsyw/D0AepwxZwQGRhjGF6XyvOjf/wvzLuUEmjbEIlAMSHGqJwQQljMfICPA6SemVFyxuP5Ume9hL3s8N7rDlpzrrWGrmlLcy6oteI866q9d311rNq7rN+LyNfZ7P1s+QbslejuWXCrdwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628234740\"\n        title=\"20210628234740\"\n        src=\"/static/aae908c841ad642cd38d97482c540e07/b3c1d/20210628234740.png\"\n        srcset=\"/static/aae908c841ad642cd38d97482c540e07/12f09/20210628234740.png 148w,\n/static/aae908c841ad642cd38d97482c540e07/e4a3f/20210628234740.png 295w,\n/static/aae908c841ad642cd38d97482c540e07/b3c1d/20210628234740.png 454w\"\n        sizes=\"(max-width: 454px) 100vw, 454px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>We are adding those two extra terms at the end to penalize the last two features. And if we are interested in penalizing all the features, we do the following:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 348px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/587bbaf5bea951046c1a930a8e7eb8de/34ea4/20210628234751.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 12.162162162162163%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsSAAALEgHS3X78AAAAZ0lEQVQI1z2OXQ7AIAiDd/9begAd4A+6OTtkyXgpJf0ajvu6EGOEqkKY0WoFbW0Vye7Mgpwz5rwhIuYZvXeUUv598/NZ2HPswpROh0IIVirmk8Ona3ZojOEZ1W/X1jxHRPYAYa2v8AW+eZo1b/OlygAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628234751\"\n        title=\"20210628234751\"\n        src=\"/static/587bbaf5bea951046c1a930a8e7eb8de/34ea4/20210628234751.png\"\n        srcset=\"/static/587bbaf5bea951046c1a930a8e7eb8de/12f09/20210628234751.png 148w,\n/static/587bbaf5bea951046c1a930a8e7eb8de/e4a3f/20210628234751.png 295w,\n/static/587bbaf5bea951046c1a930a8e7eb8de/34ea4/20210628234751.png 348w\"\n        sizes=\"(max-width: 348px) 100vw, 348px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>Using the above cost function with the extra summation, we can smooth the output of our hypothesis function to reduce overfitting. If Œª is chosen to be too large, it may smooth out the function too much and cause underfitting. Hence, we are supposed to pick and tune the value of Œª carefully.</p>\n<h2>Gradient Descent</h2>\n<p>We will modify our gradient descent function to separate Œ∏0 from the rest of the parameters because we do not want to penalize Œ∏0 which is the bias term.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 559px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a8200858321d497cc27386bee09626b7/a65ce/20210628234810.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 25%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsSAAALEgHS3X78AAAAs0lEQVQY022QCw+DIAyE/f+/U1wGTpCHjofcWqbOmF1yoRa8fmmXc4YQAsYYlG0DK6WEjWp2rbWdpZRWX8269zpueudhnYMjW2vbgL4X0NOEdV2h5BMz9a+qe9hdLZAp2UyhtSFajYnCYozNfBd8wLIse+3PMKaPMdG/udVfQnrADiEQ3QApJRH27WRqvlNKYXyNNFDjMQxn4EpDGMLaGW8K7pgq084O8fexu7t+u/p/z/oABMaG7Az8OgMAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628234810\"\n        title=\"20210628234810\"\n        src=\"/static/a8200858321d497cc27386bee09626b7/a65ce/20210628234810.png\"\n        srcset=\"/static/a8200858321d497cc27386bee09626b7/12f09/20210628234810.png 148w,\n/static/a8200858321d497cc27386bee09626b7/e4a3f/20210628234810.png 295w,\n/static/a8200858321d497cc27386bee09626b7/a65ce/20210628234810.png 559w\"\n        sizes=\"(max-width: 559px) 100vw, 559px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 382px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4416ebf97cd409d014dd01e2c8df390e/77edc/20210628234817.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 12.162162162162163%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsSAAALEgHS3X78AAAAb0lEQVQI1x1OWw7DIAzr/S86OlDzoEBC1XmB/DiW/DrMDK13LGQimE+4O4Q5cGKMgVoVdF3o8asIVCtEFc8z0e57c2bC+/5wLAOH2dxwpoS79R1ypg+G+Q7LJaOUAiJBzt/gBS10HiNkF0RhjFn3B9E0menUSqt8AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628234817\"\n        title=\"20210628234817\"\n        src=\"/static/4416ebf97cd409d014dd01e2c8df390e/77edc/20210628234817.png\"\n        srcset=\"/static/4416ebf97cd409d014dd01e2c8df390e/12f09/20210628234817.png 148w,\n/static/4416ebf97cd409d014dd01e2c8df390e/e4a3f/20210628234817.png 295w,\n/static/4416ebf97cd409d014dd01e2c8df390e/77edc/20210628234817.png 382w\"\n        sizes=\"(max-width: 382px) 100vw, 382px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>You can observe that we added the first term ‚Äú( 1 ‚Äî Œ±*(Œª/m)‚Äù which will always be less than one and that is responsible for the regularization of each parameter.</p>\n<h2>Normal Equation</h2>\n<p>To add in regularization, the equation is the same as our original, except that we add another term as such:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 247px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/896384d66bbac34c5df20f65ea08c72e/fb3c7/20210628234831.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.75675675675676%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAABWklEQVQoz3VS2W6DMBDM/39Z+tRWzVuKmnDYQAjhCMaEY7prYtdC6UjAInbHszPsQBBCQvU9mrrC6XxGGEUI6XnvFOq6RtO2KIoLZJpC9xpBECAMIyjV8ziWZTEXY0cVEiHQ0XCWSpxPJ1R1AykStHeFpqoMoUgS/NA3hpQSeZ7R4ZEjtM/dWq6Y5xnTNLv3YRhIWeEGHo+Hq31o2q6lQ51CX7IlZtRNjY/Pd3RKuW+L129neiJM4ngl9Im2xLfqhmtZmrq4ZFB69Pr+VmWFbMmqkBs8HxjjODrCLM+fKhTGaaIgFOZ14LVCvgXfR+z3b/g6HJCnAlEUU+ra+HehdL0VTBjW5n8JB62NqfeuM0FwA6O8lY6QfbVLMNFEai16f+VXqfkeWsJtEHwwp27SpzqJfQ83ydmhLSE2XrNqvnLyWdK/7Ai3sENt26B8puwf5Ne8uiCy8rr2/QJ1wVms2cplLAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628234831\"\n        title=\"20210628234831\"\n        src=\"/static/896384d66bbac34c5df20f65ea08c72e/fb3c7/20210628234831.png\"\n        srcset=\"/static/896384d66bbac34c5df20f65ea08c72e/12f09/20210628234831.png 148w,\n/static/896384d66bbac34c5df20f65ea08c72e/fb3c7/20210628234831.png 247w\"\n        sizes=\"(max-width: 247px) 100vw, 247px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2>Regularized Logistic Regression</h2>\n<h4>Cost Function</h4>\n<p>We can regularize the equation by adding a term to the end of the cost function we defined earlier in the article - Lets Learn the Fundaments of Classifying Cancer - Logistic Regression.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 433px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5f03a9426ddc5dfd33306c51f6f79345/55fc0/20210628234857.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 12.162162162162163%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsSAAALEgHS3X78AAAAa0lEQVQI112MSw6EIABDvf8ldUEAifwEGU2ANwwLF7No0qbtW57nRu87KSVyztjjwHuPMYYYAtu2IqXCWotznlI+xBhJ5znzby+VnP8OLOW60FqP0g2QmzAhxASGAVRa0Xqn/6m19vpa6+u/JMiZe5ZNOSQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628234857\"\n        title=\"20210628234857\"\n        src=\"/static/5f03a9426ddc5dfd33306c51f6f79345/55fc0/20210628234857.png\"\n        srcset=\"/static/5f03a9426ddc5dfd33306c51f6f79345/12f09/20210628234857.png 148w,\n/static/5f03a9426ddc5dfd33306c51f6f79345/e4a3f/20210628234857.png 295w,\n/static/5f03a9426ddc5dfd33306c51f6f79345/55fc0/20210628234857.png 433w\"\n        sizes=\"(max-width: 433px) 100vw, 433px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2>Gradient Descent</h2>\n<p>Similarly, we add the same regularization term ‚Äú(Œª/m) * Œ∏j‚Äù excluding the bias term which is ‚ÄúŒ∏0‚Äù. Below shows how we add these modifications to the optimization algorithm for logistic regression.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 482px; margin: 0 0 30px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/45176bc218787a4d69f4c3f670d4aafb/37e0d/20210628234927.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.64864864864865%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsSAAALEgHS3X78AAABYUlEQVQoz32S3U7DMAyF+/4PtkukaTdIiLFNG2touzZpm//04HgMaFWI5MSKnS/HTorNZoPtdovdbsfr8XhEHtM0/WuPnOUohCjRNDfEGGeBteRl/Df44RfeJwxDD601+r6H9x5KKQghIKVkM8awdV3HllL6E1o4l2CtgXOOoSEEOiRJdYO2bXk/q89Aay2vy5HSTzVFCIlBOfkO9LAmkuoBVSXQk1pFvnOeYaya9sYx4NZ4NiVJRBuQxRFwIsd+356B1YfB9V2SVdjvazw/nXA5dbhcJLVigCgVgQIuZ4vyanltao/cviIDlg/gbEJdBYID1kXousH50DDsfBphtEM+kgHeT/BuVnKYNZlmlj4M9xJinJCj1lLTydE6rr3513kCrn2D15eRyjVUusPhTRM4zr5SvqiuPNqbY5WiNFSyY3Cx9lFlF6j5gVQljEP4fsVHPEPyhUp5BPIzbBwixz8BEkMLTxkmkBoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"20210628234927\"\n        title=\"20210628234927\"\n        src=\"/static/45176bc218787a4d69f4c3f670d4aafb/37e0d/20210628234927.png\"\n        srcset=\"/static/45176bc218787a4d69f4c3f670d4aafb/12f09/20210628234927.png 148w,\n/static/45176bc218787a4d69f4c3f670d4aafb/e4a3f/20210628234927.png 295w,\n/static/45176bc218787a4d69f4c3f670d4aafb/37e0d/20210628234927.png 482w\"\n        sizes=\"(max-width: 482px) 100vw, 482px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2>Final Thoughts</h2>\n<p>That was a lot of knowledge in just one go! Don‚Äôt feel bad if you weren‚Äôt able to get all that, or felt intimidated by looking at those long Math equations. They are very simple if you have read my previous articles or have even an ounce of familiarity with previous concepts I discussed. But, if you got it all, Congrats again! Pat yourself on your back and keep moving forward! As a recap, we have covered the technical jargon that goes into data, learned about overfitting and underfitting ‚Äî their cause, and how you can resolve them. And we have also covered Regularization with respect to Linear Regression and Logistic Regression in great detail by enquiring the individual pieces that make them up ‚Äî Cost function, Gradient Descent.</p>\n<p><em>Never stop learning and trust me, you can learn anything!</em></p>\n<blockquote>\n<p>For Precious, with Patience.</p>\n</blockquote>","frontmatter":{"title":"Overfitting & Underfitting","date":"June 24, 2021","description":"Learn about overfitting and underfitting"}}},"pageContext":{"slug":"/blog/Overfitting and Underfitting/","previous":{"fields":{"slug":"/blog/Lets Learn the Fundaments of Classifying Cancer - Logistic Regression/"},"frontmatter":{"title":"Lets Learn the Fundaments of Classifying Cancer - Logistic Regression"}},"next":{"fields":{"slug":"/blog/7 Years of Programming - Here's My story/"},"frontmatter":{"title":"7 Years of Programming | Here's my Story"}}}},"staticQueryHashes":["63159454"]}